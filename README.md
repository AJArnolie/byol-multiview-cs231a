# BYOL MultiView Video Representation Learning Model - CS231A

<img width="733" alt="Screen Shot 2022-02-24 at 4 48 47 PM" src="https://user-images.githubusercontent.com/57520931/155636342-2c02e031-a11a-4964-9fac-f011f9d46061.png">

In the space of self-supervised representation learning, methods often train by attempting to reduce the difference between the representations of two separate modalities of the scene. In a recent paper, an approach to this concept was introduced that involves using video clips from different contexts as main and secondary views such that additional information can be encoded into resulting representations. In this paper, we will explore the application of these concepts to multi-view scenarios in which additional camera views can be used as additional modalities to gain a better 3D understanding of the scene, information critical for the task of 3D Object Detection. The problem of 3D Object Detection is one that is critical in supporting autonomous driving systems, and I argue that producing representations that encode spatial information about a scene using this method could potentially lead to promising results for the task. The model will learn a representation that encodes spatial information by predicting a broader, multi-view context of the scene given single-view input. The paper will then demonstrate how the resulting model performs on a downstream 3D Object Detection task on the NuScenes dataset.

Many of the concepts used in this model's design are drawn from the [BraVe](https://arxiv.org/pdf/2103.16559.pdf) and [BYOL](https://arxiv.org/pdf/2006.07733.pdf) papers. For some multi-view video input, the model sets up individual backbone networks **f** which generate representations for some main view of a scene **x_m** and for each of the secondary views **x_s**. Each of the secondary views is paired up to the main view, and the goal for each pair is for one view to regress the other. In other words, each pair is associated with a loss term which essentially minimizes the difference between these representations. Intermediate steps involving the projector network $g$ and predictor network **q** (derived from [BYOL](https://arxiv.org/pdf/2006.07733.pdf)) project each pair of representations into the same latent space such that we can directly compare the two. This approach could be applied to any number of views, but for this paper, I plan to consider three. For backbone networks **f**, I plan on starting with networks pre-trained on Kinetics 600 (provided by BraVe), and fine-tuning these networks, though I would like to try training them myself if time permits. I am open to making modifications to this model design or discussing alternate approaches to the problem. For example, I would be interested in exploring how using earlier clips or longer length clips as secondary views might impact results.

